{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN-based estimates of mosquito abundance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reads Aedes aegypti abundance data, together with mean temperature, precipitation, and relative humidity information in four neighborhoods in Puerto Rico. It then uses the weather data to create input files for an ANN trained on MoLS simulations. Finally, scaled ANN predictions are compared to the surveillance data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import various libraries and read the mosquito data file. The information is stored in a dataframe called `mdata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys, os, joblib, importlib, glob\n",
    "sys.path.append( os.path.abspath(os.path.join('..')) )\n",
    "import utils.utils as gen_utils\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data file\n",
    "mdata=pd.read_csv('../data/Mosquito_data.csv')\n",
    "# Find locations\n",
    "sites=sorted(list(set(mdata.Site)))\n",
    "print(sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create complete dataframes for each location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mdata` dataframe contains information about 4 locations (listed in `sites`). In addition, because of hurricane Maria, two weeks of weather and msoqutio data are missing. There is also one week, at the end of March 2016, where weather data is missing.\n",
    "\n",
    "We therefore need to create 4 input files with weekly data, one for each location, and interpolate the missing information over the two periods in question. Missing values are added through calls to `utils.add_missing_data` for the 2 weeks due to hurricane Maria, and to `utils.replace_nan_values` for the week of 3/2016.\n",
    "\n",
    "The resulting files are called `Arboleda_data`, `La_Margarita_data`, `Playa_data`, and `Villodas_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(gen_utils)\n",
    "# Set threshold (between 0 and 1) for zero rainfall\n",
    "#Arboleda\n",
    "Pdata = mdata[mdata[\"Site\"].isin([sites[0]])]\n",
    "Pdata.index=np.arange(Pdata.shape[0])\n",
    "Pdata.Datetime=pd.to_datetime(Pdata.Datetime)\n",
    "Pdata=gen_utils.add_missing_values(Pdata)\n",
    "Arboleda_data=gen_utils.replace_nan_values(Pdata,7)\n",
    "# La Margarita\n",
    "Pdata=mdata[mdata[\"Site\"].isin([sites[1]])]\n",
    "Pdata.index=np.arange(Pdata.shape[0])\n",
    "Pdata.Datetime=pd.to_datetime(Pdata.Datetime)\n",
    "Pdata=gen_utils.add_missing_values(Pdata)\n",
    "La_Margarita_data=gen_utils.replace_nan_values(Pdata,10)\n",
    "# Playa\n",
    "Pdata=mdata[mdata[\"Site\"].isin([sites[2]])]\n",
    "Pdata.index=np.arange(Pdata.shape[0])\n",
    "Pdata.Datetime=pd.to_datetime(Pdata.Datetime)\n",
    "Pdata=gen_utils.add_missing_values(Pdata)\n",
    "Playa_data=gen_utils.replace_nan_values(Pdata,10)\n",
    "# Villodas\n",
    "Pdata=mdata[mdata[\"Site\"].isin([sites[3]])]\n",
    "Pdata.index=np.arange(Pdata.shape[0])\n",
    "Pdata.Datetime=pd.to_datetime(Pdata.Datetime)\n",
    "Pdata=gen_utils.add_missing_values(Pdata)\n",
    "Villodas_data=gen_utils.replace_nan_values(Pdata,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the `Arboleda_data` dataframe is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Villodas_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the dataframes into separate files for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arboleda_data.to_pickle('../data/Arboleda.pd')\n",
    "La_Margarita_data.to_pickle('../data/La_Margarita.pd')\n",
    "Playa_data.to_pickle('../data/Playa.pd')\n",
    "Villodas_data.to_pickle('../data/Villodas.pd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ANN input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANN input files are dataframes whose columns are the location, year, month, day, daily average temperature, daily precipitation (in cm), daily relative humidity, and daily female abundance. They are obtained with `utils.create_AedesAI_input_dataframe`.\n",
    "* Daily temperature, relative humidity, and abundance values are estimated from normal distributions that use the weekly means and standard deviations provided in `location_data`.\n",
    "* Negative abundance values are set to 0\n",
    "* Daily precipitation values are estimated by distributing the weekly amount of rain over the entiree week, either randomly (relatively low values of `th`) or uniformly (for high values of `th`, e.g. `th=1` or even `th=0.5`)\n",
    "\n",
    "For illustration, we show a plot of the daily female abundance in Arboleda traps (`Ref` column of the `daily_Arboleda_data` dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(gen_utils)\n",
    "\n",
    "th=0.3\n",
    "loc='Arboleda'\n",
    "data_file='../data/'+loc+'.pd'; data=pd.read_pickle(data_file)\n",
    "daily_Arboleda_data, cols=gen_utils.create_AedesAI_input_dataframe(data,th)\n",
    "daily_Arboleda_data = daily_Arboleda_data.set_axis(cols, axis=1)\n",
    "daily_Arboleda_data.to_pickle('../data/Arboleda_daily.pd')\n",
    "# Extract date from the year, month, and day columns of the dataframe\n",
    "dts=np.arange(datetime(int(daily_Arboleda_data.iloc[0,1]),int(daily_Arboleda_data.iloc[0,2]),int(daily_Arboleda_data.iloc[0,3])),\n",
    "          datetime(int(daily_Arboleda_data.iloc[-1,1]), int(daily_Arboleda_data.iloc[-1,2]), int(daily_Arboleda_data.iloc[-1,3]+1)),\n",
    "          timedelta(days=1)).astype(datetime)\n",
    "# Plot daily female abundance\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(dts,daily_Arboleda_data.Ref)\n",
    "plt.title('Arboleda'); plt.xlabel(\"Days\"); plt.ylabel(\"Abundance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create daily dataframes for the other locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle('../data/La_Margarita.pd'); daily_La_Margarita_data, cols=gen_utils.create_AedesAI_input_dataframe(data,th)\n",
    "daily_La_Margarita_data = daily_La_Margarita_data.set_axis(cols, axis=1)\n",
    "daily_La_Margarita_data.to_pickle('../data/La_Margarita_daily.pd')\n",
    "\n",
    "data=pd.read_pickle('../data/Playa.pd'); daily_Playa_data, cols=gen_utils.create_AedesAI_input_dataframe(data,th)\n",
    "daily_Playa_data = daily_Playa_data.set_axis(cols, axis=1)\n",
    "daily_Playa_data.to_pickle('../data/Playa_daily.pd')\n",
    "\n",
    "data=pd.read_pickle('../data/Villodas.pd'); daily_Villodas_data,cols=gen_utils.create_AedesAI_input_dataframe(data,th)\n",
    "daily_Villodas_data = daily_Villodas_data.set_axis(cols, axis=1)\n",
    "daily_Villodas_data.to_pickle('../data/Villodas_daily.pd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ANN predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the daily weather time series created for each location as input to the trained ANN. The resulting rpedictions are then scaled and compared to the corresponding trap data.\n",
    "\n",
    "In the setup calls below,\n",
    "* The file `../utils/predictions.py` is slightly modified from the `Aedes-AI` package\n",
    "* The file `../utils/gru_avg_temp_scaler.save` was created when the network was trained. It contains the minima and maxima of the training data, which are used to normalize the input weather information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.predictions as predictions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data_shape = [90,3]\n",
    "scaler_fil = '../utils/gru_avg_temp_scaler.save'\n",
    "scaler = joblib.load(scaler_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if os.path.exists('../utils/gru_avg_temp.h5'):\n",
    "  model=tf.keras.models.load_model('../utils/gru_avg_temp.h5', custom_objects={\"r2_keras\":r2_keras})\n",
    "  print('model loaded')\n",
    "else:\n",
    "  print('uh oh')\n",
    "\n",
    "daily_datas = glob.glob('./*_daily.pd')\n",
    "for data_fil in daily_datas:\n",
    "  data = pd.read_pickle(data_fil)\n",
    "  loc = str(data.iloc[0,0])\n",
    "  outfile='./'+loc+'_gru_avg_temp_predictions.csv'\n",
    "  results=predictions.gen_preds(model, data, data_shape, scaler, fit_scaler=False, smooth=False)\n",
    "  pd.DataFrame(results, columns=['Location','Year','Month','Day','Ref','Neural Network']).to_csv(outfile,index=False)\n",
    "  print('Predictions created for '+loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8838183fa67f399c954c93f3e2ba6176d25d363535a2fcfea60ba719aaa703a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
